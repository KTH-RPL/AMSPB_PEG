# hydra.job.chdir: false
hydra:
  searchpath:
    # see https://hydra.cc/docs/advanced/search_path/
    # - file://../../cfg
    - file://cfg

headless: true
use_model: "latest" # latest, best

sim: ${task.sim}
env: ${task.env}

total_frames: 20_000_000
max_iters: -1
eval_interval: 50
render_interval: 300 # must be divisible by eval_interval, -1 if not rendering
save_interval: 100 # multiple of eval_interval
early_stop_threshold: 0.95 
early_stop_patience: 3
seed: 0

algo:
  name: ppo
  clip_param: 0.1 
  learning_rate: 5e-4 
  priv_critic: false
  gamma: 0.99

task:
  name: Evasion

  prob_old_policy: 1.0

  env:
    num_envs: 256
    arena_size: 2.0
    max_episode_length: 600
    final_separation: 0.5
    render_grid_arenas: true
    start_with_initial_velocity: false
    vel0_std: 0.15 # only if start_with_initial_velocity=True
    leave_spacing_between_envs: true
    include_noise_in_observations: false
    curriculum_pursuer_speed: true
    z_min: 0.5 # min height of the arena
    z_stop: -1.0 # below z_stop, episode is terminated

  pursuer_model: 
    name: Hummingbird
    policy: [
      # "fixed_path:hover",
      # "fixed_path:circular",
      # "pid",
      "frpn",
      ] # {fixed_path:hover, fixed_path:circular, fixed_path:lemniscate, pid ,frpn, thrust, velocity}
    source_policy: [
      # None,
      # None,
      # None,
      None,
      ]

  evader_model:
    name: Hummingbird
    policy: rate # {velocity, rate, thrust}
    source_policy: 
      None
      # evader_baseline
    controller_params: None

  do_randomization: false
  randomization:
    drone:
      train:
        mass_scale: [0.26, 1.74]
        inertia_scale: [0.026, 1.974]
        t2w_scale: [0.5556, 2.23]
        f2m_scale: [0.625, 2.5]
        drag_coef_scale: [0, 0.62]

  prev_traj_steps: -1 # Include information from the last N steps of the evader
  prev_pursuer_traj_steps: 0 # Include information from the last N steps of the pursuer

  include_distances: true
  include_closing_velocity: true
  include_last_action: false
  include_effort: false
  include_heading_to_target: false

  time_encoding: true
  use_body_rates: true

  reward_approach_weight: 0.00 
  reward_timestep: 0.007 
  reward_evader_escaped: 10
  reward_body_rates_weight: 0.0005 # befor 0.001 

  reward_out_of_bounds: 0.1 
  reward_action_smoothness: 0.0000 
  reward_heading_weight: 0.00 # if "include_heading_to_target" is True
  reward_effort_weight: 0.00 # 0.01 before # if "include_effort" is True
  
viewer:
  resolution: [960, 720]
  # eye: [30., 30., 30.] # Higher FOV but harder to interpret
  eye: [8, 0., 6.] # Automatically set by the environment based on the size
  lookat: [0., 0., 1.]
  auto: true

wandb:
  group: ${oc.select:..task.name}
  # run_name: ${oc.select:..task.name,test}-${oc.select:..algo.name,none}
  # run_name: ${oc.select:..task.name,test}-${oc.select:..algo.name,none}-${oc.select:..task.pursuer_model.policy,none}-${oc.select:..task.evader_model.policy,none}-dif:${oc.select:..task.difficulty.arena_size,0.0}-from:${oc.select:..task.initialize_from,none}
  run_name: train_evader # train_evader
  job_type: train
  entity: alesr_kth
  project: PEG
  mode: online # set to 'disabled' when debugging locally
  run_id:
  monitor_gym: True
  tags: 

defaults:
  - task: Evasion
  - algo: ppo
  - _self_