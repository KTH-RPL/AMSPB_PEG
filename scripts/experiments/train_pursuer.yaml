# hydra.job.chdir: false
hydra:
  searchpath:
    # see https://hydra.cc/docs/advanced/search_path/
    # - file://../../cfg
    - file://cfg

headless: true
use_model: "latest" # latest, best. Used when fetching the weights of the source policy.

sim: ${task.sim}
env: ${task.env}

total_frames: 15_000_000
max_iters: -1
eval_interval: 50
render_interval: 300 # must be divisible by eval_interval
save_interval: 200
early_stop_threshold: 0.95 # if for early_stop_patience evaluations the success rate is above this threshold, training is stopped
early_stop_patience: 3
seed: 0

algo:
  name: ppo
  clip_param: 0.1
  learning_rate: 5e-4
  priv_critic: false
  gamma: 0.99

task:
  name: Pursuit
  do_interception: true

  prob_old_policy: 0.66

  env:
    num_envs: 256
    arena_size: 2.0
    max_episode_length: 600
    final_separation: 0.5
    max_final_separation_track: 3.0 # if separation at the end is larger than this, negative reward for tracking
    render_grid_arenas: true
    start_with_initial_velocity: false
    vel0_std: 0.15 # only if start_with_initial_velocity=True
    leave_spacing_between_envs: true
    include_noise_in_observations: false
    z_min: 0.5 # min height of the arena
    z_stop: 0.0 # below z_stop, we assume the drone has crashed
    scale_drones: 1.0

  pursuer_model: 
    name: Hummingbird
    policy: rate # {velocity, rate, thrust}
    source_policy: None
    controller_params: None

  evader_model:
    name: Hummingbird
    # list of policies to use for the evader. policy[0:-2] are sampled with prob_old_policy, while policy[-1]
    # is sampled with 1 - prob_old_policy
    policy: [
      'fixed_path:hover', 
      'fixed_path:circular', 
      'repel', 
      # 'rate'
      ]
    source_policy: [
      None, 
      None, 
      None, 
      ]
    controller_params: None

  do_randomization: false
  randomization:
    drone:
      train:
        mass_scale: [0.26, 1.74]
        inertia_scale: [0.026, 1.974]
        t2w_scale: [0.5556, 2.23]
        f2m_scale: [0.625, 2.5]
        drag_coef_scale: [0, 0.62]

  prev_traj_steps: -1 # Include information from the last N steps of the pursuer
  prev_evader_traj_steps: 0 # Include information from the last N steps of the evader

  include_distances: true
  include_closing_velocity: true
  include_last_action: false
  include_effort: false
  include_heading_to_target: false

  time_encoding: true

  use_body_rates: true
  use_timestep_reward: false

  reward_timestep: 0.000 # if "use_timestep_reward" is True
  reward_approach_weight: 0.5 
  reward_evader_caught: 10
  reward_body_rates_weight: 0.005
  reward_out_of_bounds: 0.1 
  reward_action_smoothness: 0.0000 
  reward_heading_weight: 0.00 # if "include_heading_to_target" is True
  reward_effort_weight: 0.000 # if "include_effort" is True
  
viewer:
  resolution: [960, 720]
  # eye: [30., 30., 30.] # Higher FOV but harder to interpret
  # eye: [8, 0., 6.]
  auto: true
  eye: [14., 0., 8.] # Automatically set by the environment based on the size
  lookat: [0., 0., 1.]

wandb:
  group: ${oc.select:..task.name}
  run_name: train_pursuer 
  job_type: train
  entity: 
  project: 
  mode: online # set to 'disabled' when debugging locally
  run_id:
  monitor_gym: True
  tags: 

defaults:
  - task: Pursuit
  - algo:
      ppo 
  - _self_