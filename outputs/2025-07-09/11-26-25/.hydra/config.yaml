task:
  env:
    num_envs: 1
    env_spacing: 6
    max_episode_length: 10000
    arena_size: 3.0
    final_separation: 0.5
    max_final_separation_track: 3.0
    render_grid_arenas: true
    start_with_initial_velocity: false
    vel0_std: 0.0
    leave_spacing_between_envs: true
    include_noise_in_observations: false
    z_min: 0.5
    z_stop: 0.2
  sim:
    dt: 0.016
    substeps: 1
    gravity:
    - 0
    - 0
    - -9.81
    replicate_physics: false
    use_flatcache: true
    use_gpu_pipeline: true
    device: cuda:0
    solver_type: 1
    use_gpu: true
    bounce_threshold_velocity: 0.2
    friction_offset_threshold: 0.04
    friction_correlation_distance: 0.025
    enable_stabilization: true
    gpu_max_rigid_contact_count: 524288
    gpu_max_rigid_patch_count: 163840
    gpu_found_lost_pairs_capacity: 4194304
    gpu_found_lost_aggregate_pairs_capacity: 33554432
    gpu_total_aggregate_pairs_capacity: 4194304
    gpu_max_soft_body_contacts: 1048576
    gpu_max_particle_contacts: 1048576
    gpu_heap_capacity: 67108864
    gpu_temp_buffer_capacity: 16777216
    gpu_max_num_partitions: 8
  name: Pursuit
  drone_model:
    name: Firefly
    controller: LeePositionController
  time_encoding: true
  reward_effort_weight: 0.01
  reward_action_smoothness_weight: 0.0
  reward_motion_smoothness_weight: 0.0
  reward_distance_scale: 1.2
  action_transform: null
  do_interception: true
  prob_old_policy: 1.0
  pursuer_model:
    name: Hummingbird
    policy: velocity
    source_policy: None
    controller_params: None
  evader_model:
    name: Hummingbird
    policy:
    - rate
    - repel_dumb
    source_policy:
    - e-rate_s2-rate_s2-sz_3.0-len_750-v0_0-br_1-prob_0.75-dist_1-p_a_0-ht_0-pid_1-frpn_1-u_l-seed_0-fr_rate_s2
    - None
    controller_params: None
  do_randomization: false
  randomization:
    drone:
      train:
        mass_scale:
        - 0.26
        - 1.74
        inertia_scale:
        - 0.026
        - 1.974
        t2w_scale:
        - 0.5556
        - 2.23
        f2m_scale:
        - 0.625
        - 2.5
        drag_coef_scale:
        - 0
        - 0.62
  prev_traj_steps: -1
  prev_evader_traj_steps: 4
  include_distances: true
  include_last_action: false
  include_effort: false
  include_heading_to_target: false
  use_body_rates: true
  use_timestep_reward: false
  reward_timestep: 0.005
  reward_approach_weight: 0.5
  reward_evader_caught: 5
  reward_body_rates_weight: 0.005
  reward_out_of_bounds: 1
  reward_action_smoothness: 0.0
  reward_heading_weight: 0.01
algo:
  name: ppo
  train_every: 32
  ppo_epochs: 4
  num_minibatches: 16
  clip_param: 0.1
  learning_rate: 0.0005
  gamma: 0.99
  priv_actor: false
  priv_critic: false
  checkpoint_path: null
headless: false
use_model: latest
sim: ${task.sim}
env: ${task.env}
total_frames: 5000000
max_iters: -1
eval_interval: 50
render_interval: 200
save_interval: 200
early_stop_threshold: 0.95
early_stop_patience: 3
seed: 0
viewer:
  resolution:
  - 1920
  - 1080
  eye:
  - 14
  - 0.0
  - 10.0
  lookat:
  - 0.0
  - 0.0
  - 1.0
wandb:
  group: ${oc.select:..task.name}
  run_name: benchmark_pursuer
  job_type: evaluate
  entity: alesr_kth
  project: PEG
  mode: online
  run_id: null
  monitor_gym: true
  tags: null
