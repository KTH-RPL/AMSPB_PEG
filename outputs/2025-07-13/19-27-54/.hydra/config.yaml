task:
  env:
    num_envs: 256
    env_spacing: 6
    max_episode_length: 600
    arena_size: 2.0
    final_separation: 0.5
    render_grid_arenas: true
    start_with_initial_velocity: false
    vel0_std: 0.15
    leave_spacing_between_envs: true
    include_noise_in_observations: false
    curriculum_pursuer_speed: true
    z_min: 0.5
    z_stop: -1.0
  sim:
    dt: 0.016
    substeps: 1
    gravity:
    - 0
    - 0
    - -9.81
    replicate_physics: false
    use_flatcache: true
    use_gpu_pipeline: true
    device: cuda:0
    solver_type: 1
    use_gpu: true
    bounce_threshold_velocity: 0.2
    friction_offset_threshold: 0.04
    friction_correlation_distance: 0.025
    enable_stabilization: true
    gpu_max_rigid_contact_count: 524288
    gpu_max_rigid_patch_count: 163840
    gpu_found_lost_pairs_capacity: 4194304
    gpu_found_lost_aggregate_pairs_capacity: 33554432
    gpu_total_aggregate_pairs_capacity: 4194304
    gpu_max_soft_body_contacts: 1048576
    gpu_max_particle_contacts: 1048576
    gpu_heap_capacity: 67108864
    gpu_temp_buffer_capacity: 16777216
    gpu_max_num_partitions: 8
  name: Evasion
  drone_model:
    name: Firefly
    controller: LeePositionController
  time_encoding: true
  reward_effort_weight: 0.0
  reward_action_smoothness_weight: 0.0
  reward_motion_smoothness_weight: 0.0
  reward_distance_scale: 1.2
  action_transform: null
  prob_old_policy: 1.0
  pursuer_model:
    name: Hummingbird
    policy:
    - frpn
    source_policy:
    - None
  evader_model:
    name: Hummingbird
    policy: rate
    source_policy: None
    controller_params: None
  do_randomization: false
  randomization:
    drone:
      train:
        mass_scale:
        - 0.26
        - 1.74
        inertia_scale:
        - 0.026
        - 1.974
        t2w_scale:
        - 0.5556
        - 2.23
        f2m_scale:
        - 0.625
        - 2.5
        drag_coef_scale:
        - 0
        - 0.62
  prev_traj_steps: -1
  prev_pursuer_traj_steps: 0
  include_distances: true
  include_closing_velocity: true
  include_last_action: false
  include_effort: false
  include_heading_to_target: false
  use_body_rates: true
  reward_approach_weight: 0.0
  reward_timestep: 0.007
  reward_evader_escaped: 10
  reward_body_rates_weight: 0.0005
  reward_out_of_bounds: 0.1
  reward_action_smoothness: 0.0
  reward_heading_weight: 0.0
algo:
  name: ppo
  train_every: 32
  ppo_epochs: 4
  num_minibatches: 16
  clip_param: 0.1
  learning_rate: 0.0005
  gamma: 0.99
  priv_actor: false
  priv_critic: false
  checkpoint_path: null
headless: true
use_model: latest
sim: ${task.sim}
env: ${task.env}
total_frames: 20000000
max_iters: -1
eval_interval: 50
render_interval: 300
save_interval: 100
early_stop_threshold: 0.95
early_stop_patience: 3
seed: 0
viewer:
  resolution:
  - 960
  - 720
  eye:
  - 8
  - 0.0
  - 6.0
  lookat:
  - 0.0
  - 0.0
  - 1.0
wandb:
  group: ${oc.select:..task.name}
  run_name: train_evader_repeat
  job_type: train
  entity: alesr_kth
  project: PEG
  mode: online
  run_id: null
  monitor_gym: true
  tags: null
